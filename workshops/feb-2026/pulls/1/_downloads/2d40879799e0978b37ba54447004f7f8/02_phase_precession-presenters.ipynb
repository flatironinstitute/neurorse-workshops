{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22826453",
   "metadata": {
    "tags": [
     "hide-input",
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"plotting functions contained within `_documentation_utils` are intended for nemos's documentation.\",\n",
    "    category=UserWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf00b5c",
   "metadata": {},
   "source": [
    ":::{admonition} Download\n",
    ":class: important render-all\n",
    "\n",
    "This notebook can be downloaded as **{nb-download}`02_phase_precession-presenters.ipynb`**. See the button at the top right to download as markdown or pdf.\n",
    ":::\n",
    "\n",
    "# Analyzing hippocampal place cells with Pynapple and NeMoS\n",
    "This notebook has had all its explanatory text removed and has not been run.\n",
    " It is intended to be downloaded and run locally (or on the provided binder)\n",
    " while listening to the presenter's explanation. In order to see the fully\n",
    " rendered of this notebook, go [here](../../full/group_projects/02_phase_precession.md)\n",
    "\n",
    "\n",
    "    \n",
    "In this tutorial we will learn how to use more advanced applications of pynapple: signal processing and decoding. We'll apply these methods to demonstrate and visualize some well-known physiological properties of hippocampal activity, specifically phase presession of place cells and sequential coordination of place cell activity during theta oscillations.\n",
    "\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "\n",
    "    \n",
    "For part 1 of this notebook, we will be using Pynapple to achieve the following objectives:\n",
    "1. Load in and get a feel for the data set   \n",
    "2. Identify and extract theta oscillations in the LFP\n",
    "3. Identify place cells using 1D tuning curves\n",
    "4. Visualize phase precession using 2D tuning curves\n",
    "5. Use Baysian decoding to reconstruct spatial sequences from population activity\n",
    "\n",
    "For part 2, we will by applying NeMoS to explore the dataset further by:\n",
    "1. Visualize speed vs. position encoding\n",
    "2. Create a design matrix using a basis set to simplify speed and position parameter space\n",
    "3. Fit a Poisson GLM to neural activity with speed and position as predictors\n",
    "4. Evaluate the model's predicted tuning curves and compare to the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d11c4",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "import workshop_utils\n",
    "# imports\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import pynapple as nap\n",
    "\n",
    "# necessary for animation\n",
    "import nemos as nmo\n",
    "plt.style.use(nmo.styles.plot_style)\n",
    "\n",
    "# configure pynapple to ignore conversion warning\n",
    "nap.nap_config.suppress_conversion_warnings = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16726dc9",
   "metadata": {},
   "source": [
    "## Part 1: Using Pynapple to identify phase precession and hippocampal sequences\n",
    "\n",
    "### Fetching the data\n",
    "\n",
    "\n",
    "    \n",
    "The data set we'll be looking at is from the manuscript [Diversity in neural firing dynamics supports both rigid and learned hippocampal sequences](https://www.science.org/doi/10.1126/science.aad1935). In this study, the authors collected electrophisiology data in rats across multiple sites in layer CA1 of hippocampus to extract the LFP alongside spiking activity of many simultaneous pyramidal units. In each recording session, data were collected while the rats explored a novel environment (a linear or circular track), as well as during sleep before and after exploration. In our following analyses, we'll focus on the exploration period of a single rat and recording session.\n",
    "\n",
    "The full dataset for this study can be accessed on [DANDI](https://dandiarchive.org/dandiset/000044/0.210812.1516). Since the file size of a recording session can be large from the LFP saved for each recorded channel, we'll use a smaller file that contains the spiking activity and the LFP from a single, representative channel, which is hosted on [OSF](https://osf.io/2dfvp). This smaller file, like the original data, is saved as an [NWB](https://www.nwb.org) file.\n",
    "\n",
    "If you ran the workshop setup script, you should have this file downloaded already. If not, the function we'll use to fetch it will download it for you. This function is called `fetch_data`, and can be imported from the `workshop_utils` module. This function will give us the file path to where the data is stored. We can then use the pynapple function `load_file` to load in the data, which is able to handle the NWB file type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62351804",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "# fetch file path\n",
    "path = workshop_utils.fetch_data(\"Achilles_10252013_EEG.nwb\")\n",
    "# load data with pynapple\n",
    "data = nap.load_file(path)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a311e2",
   "metadata": {},
   "source": [
    "    \n",
    "This returns a dictionary of pynapple objects that have been extracted from the NWB file. Let's explore each of these objects.\n",
    "\n",
    "\n",
    "\n",
    ":::{admonition} Note\n",
    ":class: note render-all\n",
    "We will ignore the object `theta_phase` because we will be computing this ourselves later on in the exercise.\n",
    ":::\n",
    "\n",
    "#### units\n",
    "\n",
    "\n",
    "    \n",
    "The `units` field is a [`TsGroup`](https://pynapple.org/generated/pynapple.TsGroup.html#pynapple.TsGroup): a collection of [`Ts`](https://pynapple.org/generated/pynapple.Ts.html#pynapple.Ts) objects containing the spike times of each unit, where the \"Index\" is the unit number or key. Each unit has the following metadata:\n",
    "- **rate**: computed by pynapple, is the average firing rate of the neuron across all recorded time points.\n",
    "- **location**, **shank**, and **cell_type**: variables saved and imported from the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa04641",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "data[\"units\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb284fa",
   "metadata": {},
   "source": [
    "We can access the spike times of a single unit by indexing the `TsGroup` by its unit number. For example, to access the spike times of unit 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f512e59",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "data[\"units\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285ffd8",
   "metadata": {},
   "source": [
    "#### rem, nrem, and forward_ep\n",
    "\n",
    "\n",
    "\n",
    "The next three objects; `rem`, `nrem`, and `forward_ep`; are all [`IntervalSet`](https://pynapple.org/generated/pynapple.IntervalSet.html#pynapple.IntervalSet) objects containing time windows of REM sleep, nREM sleep, and forward runs down the linear maze, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9ab2b",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "data[\"rem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecc00b",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "data[\"nrem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f74d3b",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "data[\"forward_ep\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc48b5",
   "metadata": {},
   "source": [
    "All intervals in `forward_ep` occur in the middle of the session, while `rem` and `nrem` both contain sleep epochs that occur before and after exploration. \n",
    "    \n",
    "The following plot demonstrates how each of these labelled epochs are organized across the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650679f",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "t_start = data[\"nrem\"].start[0]\n",
    "fig,ax = plt.subplots(figsize=(10,2), constrained_layout=True)\n",
    "sp1 = [ax.axvspan((iset.start[0]-t_start)/60, (iset.end[0]-t_start)/60, color=\"blue\", alpha=0.1) for iset in data[\"rem\"]];\n",
    "sp2 = [ax.axvspan((iset.start[0]-t_start)/60, (iset.end[0]-t_start)/60, color=\"green\", alpha=0.1) for iset in data[\"nrem\"]];\n",
    "sp3 = [ax.axvspan((iset.start[0]-t_start)/60, (iset.end[0]-t_start)/60, color=\"red\", alpha=0.1) for iset in data[\"forward_ep\"]];\n",
    "ax.set(xlabel=\"Time within session (minutes)\", title=\"Labelled time intervals across session\", yticks=[])\n",
    "ax.legend([sp1[0],sp2[0],sp3[0]], [\"REM sleep\",\"nREM sleep\",\"forward runs\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a99059",
   "metadata": {},
   "source": [
    "#### eeg\n",
    "\n",
    "\n",
    "\n",
    "The `eeg` object is a [`TsdFrame`](https://pynapple.org/generated/pynapple.TsdFrame.html#pynapple.TsdFrame) containing an LFP voltage trace for a single representative channel in CA1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a0c7e",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "data[\"eeg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd3a83",
   "metadata": {},
   "source": [
    "Despite having a single column, this [`TsdFrame`](https://pynapple.org/generated/pynapple.TsdFrame.html#pynapple.TsdFrame) is still a 2D object. We can represent this as a 1D `Tsd` by indexing into the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9de2c5",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "data[\"eeg\"][:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49832a",
   "metadata": {},
   "source": [
    "#### position\n",
    "\n",
    "\n",
    "\n",
    "The final object, `position`, is a [`Tsd`](https://pynapple.org/generated/pynapple.Tsd.html#pynapple.Tsd) containing the linearized position of the animal, in centimeters, recorded during the exploration window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b24aaf",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "data[\"position\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68af4f",
   "metadata": {},
   "source": [
    "Positions that are not defined, i.e. when the animal is at rest, are filled with `NaN`.\n",
    "\n",
    "This object additionally contains a [`time_support`](https://pynapple.org/generated/pynapple.Tsd.html#id1) attribute, which gives the time interval during which positions are recorded (including points recorded as `NaN`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb2e43",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "data[\"position\"].time_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e10356",
   "metadata": {},
   "source": [
    "Let's visualize the first 300 seconds of position data and overlay `forward_ep` intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29422a93",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "pos_start = data[\"position\"].time_support.start[0]\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "l1 = ax.plot(data[\"position\"])\n",
    "l2 = [ax.axvspan(iset.start[0], iset.end[0], color=\"red\", alpha=0.1) for iset in data[\"forward_ep\"]];\n",
    "ax.set(xlim=[pos_start,pos_start+300], ylabel=\"Position (cm)\", xlabel=\"Time (s)\", title=\"Tracked position along linear maze\")\n",
    "ax.legend([l1[0], l2[0]], [\"animal position\", \"forward run epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a4c57",
   "metadata": {},
   "source": [
    "This plot confirms that positions are only recorded while the animal is moving along the track. Additionally, it is clear that the intervals in `forward_ep` capture only perios when the animal's position is increasing, during forward runs.\n",
    "\n",
    "We'll save out the following variables that we'll need throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7bd58",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "position = data[\"position\"]\n",
    "lfp = data[\"eeg\"][:,0]\n",
    "spikes = data[\"units\"]\n",
    "forward_ep = data[\"forward_ep\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369aeb5",
   "metadata": {},
   "source": [
    "### Restricting and visualizing the data\n",
    "\n",
    "\n",
    "\n",
    "For the following exercises, we'll only focus on periods when the animal is awake and running. We can get this information from `position`.\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Save out the time support of `position`, giving us the epoch during which the animal is awake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "awake_ep = position.time_support\n",
    "awake_ep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957309c6",
   "metadata": {},
   "source": [
    "You may have noticed many `nan` values for position during the awake period; these values correspond to when the animals is at rest. We also want, then, epochs describing periods when the animal is running. Some of this information is saved already in `forward_ep`. \n",
    "\n",
    "\n",
    "\n",
    "#### 2. Confirm that when restricting position to `forward_ep`, there are no `nan` values in position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8821b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict position and check for nans\n",
    "np.any(np.isnan(position.restrict(forward_ep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680ac71",
   "metadata": {},
   "source": [
    "What if we want *all* movement epochs, not just forward runs? We can derive this from `position` by dropping all `nan` values and recomputing the time support. \n",
    "\n",
    "\n",
    "\n",
    "#### 3. Extract time intervals from `position` using the [`dropna`](https://pynapple.org/generated/pynapple.Tsd.dropna.html) and [`find_support`](https://pynapple.org/generated/pynapple.Tsd.find_support.html) methods.\n",
    "\n",
    "\n",
    "\n",
    "- Use `min_gap` of 1 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc542dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan values\n",
    "pos_good = data[\"position\"].dropna()\n",
    "run_ep = pos_good.find_support(1)\n",
    "run_ep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c3e8f",
   "metadata": {},
   "source": [
    "Finally, we can use `run_ep` and `forward_ep` to extract epochs when the animal is running backwards.\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Use the `IntervalSet` method [`set_diff`](https://pynapple.org/generated/pynapple.IntervalSet.set_diff.html) to get `backward_ep` from `run_ep` and `forward_ep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_ep = run_ep.set_diff(forward_ep)\n",
    "backward_ep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e53e829",
   "metadata": {},
   "source": [
    "    \n",
    "Now, when extracting the LFP, spikes, and position, we can use `restrict()` with any of these epochs to restrict the data to our movement period of interest.\n",
    "\n",
    "To get a sense of what the LFP looks like while the animal runs down the linear track, we can plot each variable, `lfp` and `position`, side-by-side. Let's do this for an example run; specifically, we'll look at forward run 9.\n",
    "\n",
    "\n",
    "\n",
    "#### 5. Create an interval set for forward run index 9, adding 2 seconds to the end of the interval. Restrict `lfp` and `position` to this epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_ep = nap.IntervalSet(start=forward_ep[9].start, end=forward_ep[9].end+2)\n",
    "ex_lfp = lfp.restrict(ex_ep)\n",
    "ex_position = position.restrict(ex_ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa5f03a",
   "metadata": {},
   "source": [
    "Let's plot the example LFP trace and anmimal position. Plotting `Tsd` objects will automatically put time on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17422440",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, constrained_layout=True, figsize=(10, 4), sharex=True)\n",
    "\n",
    "# plot LFP\n",
    "axs[0].plot(ex_lfp)\n",
    "axs[0].set_title(\"Local Field Potential on Linear Track\")\n",
    "axs[0].set_ylabel(\"LFP (a.u.)\")\n",
    "\n",
    "# plot animal's position\n",
    "axs[1].plot(ex_position)\n",
    "axs[1].set_title(\"Animal Position on Linear Track\")\n",
    "axs[1].set_ylabel(\"Position (cm)\") # LOOK UP UNITS\n",
    "axs[1].set_xlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db1ee7",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3519e",
   "metadata": {},
   "source": [
    "As we would expect, there is a strong theta oscillation dominating the LFP while the animal runs down the track. This oscillation is weaker after the run is complete.\n",
    "\n",
    "\n",
    "\n",
    "### Getting the Wavelet Decomposition\n",
    "\n",
    "\n",
    "\n",
    "To illustrate this further, we'll perform a wavelet decomposition on the LFP trace during this run. We can do this in pynapple using the function [`nap.compute_wavelet_transform`](https://pynapple.org/generated/pynapple.process.wavelets.html#pynapple.process.wavelets.compute_wavelet_transform).\n",
    "\n",
    "A [continuous wavelet transform](https://en.wikipedia.org/wiki/Continuous_wavelet_transform) decomposes a signal into a set of [wavelets](https://en.wikipedia.org/wiki/Wavelet), in this case [Morlet wavelets](https://en.wikipedia.org/wiki/Morlet_wavelet), that span both frequency and time. You can think of the wavelet transform as a cross-correlation between the signal and each wavelet, giving the similarity between the signal and various frequency components at each time point of the signal. Similar to a Fourier transform, this gives us an estimate of what frequencies are dominating a signal. Unlike the Fourier tranform, however, the wavelet transform gives us this estimate as a function of time.\n",
    "\n",
    "We must define the frequency set that we'd like to use for our decomposition. We can do this with the numpy function [`np.geomspace`](https://numpy.org/doc/stable/reference/generated/numpy.geomspace.html), which returns numbers evenly spaced on a log scale. We pass the lower frequency, the upper frequency, and number of samples as positional arguments.\n",
    "\n",
    "\n",
    "\n",
    "#### 6. Define 100 log-spaced samples between 5 and 200 Hz using [`np.geomspace`](https://numpy.org/doc/stable/reference/generated/numpy.geomspace.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ad580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 log-spaced samples between 5Hz and 200Hz\n",
    "freqs = np.geomspace(5, 200, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3034279",
   "metadata": {},
   "source": [
    "We can now compute the wavelet transform on our LFP data during the example run using [`nap.compute_wavelet_transform`](https://pynapple.org/generated/pynapple.process.wavelets.html#pynapple.process.wavelets.compute_wavelet_transform) by passing both `ex_lfp` and `freqs`. We'll also pass the optional argument `fs`, which is known to be 1250Hz from the study methods.\n",
    "\n",
    "\n",
    "\n",
    "#### 7. Compute the wavelet transform of `ex_lfp` using `freqs` defined above.\n",
    "\n",
    "\n",
    "\n",
    "- Supply the known sampling rate, 1250 Hz, as the optional argument `fs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ece47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 1250\n",
    "ex_cwt = nap.compute_wavelet_transform(ex_lfp, freqs, fs=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e218cda",
   "metadata": {},
   "source": [
    ":::{admonition} Note\n",
    ":class: tip render-all\n",
    "If `fs` is not provided, it can be inferred from the time series [`rate`](https://pynapple.org/generated/pynapple.Tsd.html#id0) attribute, e.g. `ex_lfp.rate`. However, while inferred rate is close to the true sampling rate, it can introduce a small floating-point error. Therefore, it is better to supply the true sampling rate when it is known.\n",
    ":::\n",
    "\n",
    "\n",
    "    \n",
    "We can visualize the results by plotting a heat map of the calculated wavelet scalogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a8386",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10,4), constrained_layout=True, height_ratios=[1.0, 0.3], sharex=True)\n",
    "fig.suptitle(\"Wavelet Decomposition\")\n",
    "\n",
    "amp = np.abs(ex_cwt.values)\n",
    "cax = axs[0].pcolormesh(ex_cwt.t, freqs, amp.T)\n",
    "axs[0].set(ylabel=\"Frequency (Hz)\", yscale='log', yticks=freqs[::10], yticklabels=np.rint(freqs[::10]));\n",
    "axs[0].minorticks_off()\n",
    "fig.colorbar(cax,label=\"Amplitude\")\n",
    "\n",
    "p1 = axs[1].plot(ex_lfp)\n",
    "axs[1].set(ylabel=\"LFP (a.u.)\", xlabel=\"Time(s)\")\n",
    "axs[1].margins(0)\n",
    "ax = axs[1].twinx()\n",
    "p2 = ax.plot(ex_position, color=\"orange\")\n",
    "ax.set_ylabel(\"Position (cm)\")\n",
    "ax.legend([p1[0], p2[0]],[\"raw LFP\",\"animal position\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365161c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-02.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446fce8",
   "metadata": {},
   "source": [
    "    \n",
    "You should see a strong presence of theta in the 6-12Hz frequency band while the animal runs down the track, which dampens during rest.\n",
    "\n",
    "\n",
    "\n",
    "### Filtering for theta\n",
    "\n",
    "\n",
    "\n",
    "For the remaining exercises, we'll reduce our example epoch to the portion when the animal is running forward along the linear track.\n",
    "\n",
    "\n",
    "\n",
    "#### 8. Restrict the `lfp` and `position` to `forward_ep` and create a new `IntervalSet` for forward run index 9 with no padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a525ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp = lfp.restrict(forward_ep)\n",
    "position = position.restrict(forward_ep)\n",
    "ex_run_ep = nap.IntervalSet(start=forward_ep[9].start, end=forward_ep[9].end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a7b48",
   "metadata": {},
   "source": [
    "We can extract the theta oscillation by applying a bandpass filter on the raw LFP. To do this, we use the pynapple function [`nap.apply_bandpass_filter`](https://pynapple.org/generated/pynapple.process.filtering.html#pynapple.process.filtering.apply_bandpass_filter). Conveniently, this function will recognize and handle splits in the epoched data (i.e. applying the filtering separately to discontinuous epochs), so we don't have to worry about passing signals that have been split in time.\n",
    "\n",
    "\n",
    "\n",
    "#### 9. Using [`nap.apply_bandpass_filter`](https://pynapple.org/generated/pynapple.process.filtering.html#pynapple.process.filtering.apply_bandpass_filter), filter `lfp` for theta within a 6-12 Hz range.\n",
    "\n",
    "\n",
    "\n",
    "- Same as before, pass the sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9800b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_band = nap.apply_bandpass_filter(lfp, (6.0, 12.0), fs=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a23e01",
   "metadata": {},
   "source": [
    "We can visualize the output by plotting the filtered signal with the original signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c4e38",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(10, 3))\n",
    "plt.plot(lfp.restrict(ex_run_ep), label=\"raw\")\n",
    "plt.plot(theta_band.restrict(ex_run_ep), label=\"filtered\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"LFP (a.u.)\")\n",
    "plt.title(\"Bandpass filter for theta oscillations (6-12 Hz)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7973009",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-03.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2329cb1",
   "metadata": {},
   "source": [
    "### Computing theta phase\n",
    "\n",
    "\n",
    "\n",
    "In order to examine phase precession in place cells, we need to extract the phase of theta from the filtered signal. We can do this by taking the angle of the [Hilbert transform](https://en.wikipedia.org/wiki/Hilbert_transform).\n",
    "\n",
    "#### 10. Use [`sp.signal.hilbert`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.hilbert.html) to perform the Hilbert transform of `theta_band`, using [`np.angle`](https://numpy.org/doc/2.3/reference/generated/numpy.angle.html) to extract the angle. Convert the output angle to a [0, 2pi] range, and store the result in a `Tsd` object.\n",
    "\n",
    "- TIP: don't forget to pass the time support!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d666398",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = np.angle(sp.signal.hilbert(theta_band)) # compute phase with hilbert transform\n",
    "phase[phase < 0] += 2 * np.pi # wrap to [0,2pi]\n",
    "theta_phase = nap.Tsd(t=theta_band.t, d=phase, time_support=theta_band.time_support)\n",
    "theta_phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42056a06",
   "metadata": {},
   "source": [
    "Let's plot the phase on top of the filtered LFP signal, zooming in on a few cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400554bb",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "ex_run_shorter = nap.IntervalSet(ex_run_ep.start[0], ex_run_ep.start[0]+0.5)\n",
    "fig,axs = plt.subplots(2,1,figsize=(10,4), constrained_layout=True, sharex=True)#, height_ratios=[2,1])\n",
    "ax = axs[0]\n",
    "ax.plot(lfp.restrict(ex_run_shorter))\n",
    "ax.set_ylabel(\"LFP (a.u.)\")\n",
    "ax = axs[1]\n",
    "p1 = ax.plot(theta_phase.restrict(ex_run_shorter), color='r')\n",
    "ax.set_ylabel(\"Phase (rad)\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax = ax.twinx()\n",
    "p2 = ax.plot(theta_band.restrict(ex_run_shorter))\n",
    "ax.set_ylabel(\"Filtered LFP (a.u.)\")\n",
    "ax.legend([p1[0],p2[0]],[\"theta phase\",\"filtered LFP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd2a3e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-04.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92348cea",
   "metadata": {},
   "source": [
    "You should be able to see that each cycle \"resets\" (i.e. goes from $2\\pi$ to $0$) at peaks of the theta oscillation.\n",
    "\n",
    "\n",
    "\n",
    "### Computing 1D tuning curves: place fields\n",
    "\n",
    "\n",
    "\n",
    "In order to identify phase precession in single units, we need to know their place selectivity. We can find place firing preferences of each unit by using the function [`nap.compute_tuning_curves`](https://pynapple.org/generated/pynapple.process.tuning_curves.html#pynapple.process.tuning_curves.compute_tuning_curves).\n",
    "\n",
    "First, we'll filter for units that fire at least 1 Hz and at most 10 Hz when the animal is running forward along the linear track. This will select for units that are active during our window of interest and eliminate putative interneurons (i.e. fast-firing inhibitory neurons that don't usually have place selectivity). Afterwards, we'll compute the tuning curves for these sub-selected units over position.\n",
    "\n",
    "\n",
    "\n",
    "#### 11. Restrict `spikes` to `forward_ep` and select for units whose rate is at least 1 Hz and at most 10 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7491752",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_spikes = spikes[(spikes.restrict(forward_ep).rate >= 1) & (spikes.restrict(forward_ep).rate <= 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a293a3",
   "metadata": {},
   "source": [
    "#### 12. Compute tuning curves for units in `good_spikes` with respect to `position`\n",
    "\n",
    "\n",
    "\n",
    "- Use 50 position bins\n",
    "- Name the feature `\"position\"` using the optional argument `feature_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4160b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_fields = nap.compute_tuning_curves(good_spikes, position, 50, feature_names=[\"position\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc8dbb",
   "metadata": {},
   "source": [
    "This function returns tuning curves as an `xarray.DataArray`, with coordinates for unit (first dimension) and position (second dimension). An `xarray.DataArray` object provides convenient tools for plotting and other manipulations, and it scales well for tuning curves with more than 1 feature. \n",
    "\n",
    "\n",
    "\n",
    ":::{admonition} Tip\n",
    ":class: tip render-all\n",
    "\n",
    "The reason [`nap.compute_tuning_curves`](https://pynapple.org/generated/pynapple.process.tuning_curves.html#pynapple.process.tuning_curves.compute_tuning_curves) returns a `xarray.DataArray` and not a Pynapple object is because the array elements no longer correspond to *time*, which Pynapple objects require.\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "We can use the `xarray.DataArray` `plot` method to easily plot each unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ba92f",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# smooth the place fields so they look nice\n",
    "place_fields.data = gaussian_filter1d(place_fields.data, 1, axis=-1)\n",
    "\n",
    "p = place_fields.plot(x=\"position\", col=\"unit\", col_wrap=5, size=1.2, sharey=False)\n",
    "p.set_ylabels(\"firing rate (Hz)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a93b9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p.fig.savefig(\"../../_static/_check_figs/02-05.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3370fa",
   "metadata": {},
   "source": [
    "    \n",
    "We can see spatial selectivity in many of the units; across the population, we have firing fields tiling the entire linear track. \n",
    "\n",
    "\n",
    "\n",
    "### Visualizing phase precession within a single unit\n",
    "\n",
    "\n",
    "    \n",
    "As an initial visualization of phase precession, we'll look at a single traversal of the linear track. First, let's look at how the timing of an example unit's spikes lines up with the LFP and theta. To plot the spike times on the same axis as the LFP, we'll use the pynapple object's method [`value_from`](https://pynapple.org/generated/pynapple.TsGroup.value_from.html) to align the spike times with the theta amplitude. For our spiking data, this will find the amplitude closest in time to each spike. Let's start by applying [`value_from`](https://pynapple.org/generated/pynapple.TsGroup.value_from.html) on unit 177, who's place field is cenetered on the linear track, using `theta_band` to align the amplityde of the filtered LFP.\n",
    "\n",
    "#### 13. Use the pynapple object method [`value_from`](https://pynapple.org/generated/pynapple.TsGroup.value_from.html) to find the value of `theta_band` corresponding to each spike time from unit 177."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = 177\n",
    "spike_theta = spikes[unit].value_from(theta_band)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233aede5",
   "metadata": {},
   "source": [
    "Let's plot `spike_theta` on top of the LFP and filtered theta, as well as visualize the animal's position along the track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03078355",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2, 1, figsize=(10,4), constrained_layout=True, sharex=True)\n",
    "axs[0].plot(lfp.restrict(ex_run_ep), alpha=0.5, label=\"raw LFP\")\n",
    "axs[0].plot(theta_band.restrict(ex_run_ep), color=\"slateblue\", label=\"filtered theta\")\n",
    "axs[0].plot(spike_theta.restrict(ex_run_ep), 'o', color=\"orange\", label=\"spike times\")\n",
    "axs[0].set(ylabel=\"LFP (a.u.)\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(ex_position, '--', color=\"green\", label=\"animal position\")\n",
    "axs[1].plot(ex_position[(ex_position > 50).values & (ex_position < 130).values], color=\"green\", lw=3, label=\"place field bounds\")\n",
    "axs[1].set(ylabel=\"Position (cm)\", xlabel=\"Time (s)\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523753e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-06.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81a986",
   "metadata": {},
   "source": [
    "    \n",
    "As the animal runs through unit 177's place field (thick green), the unit spikes (orange dots) at specific points along the theta cycle dependent on position: starting at the rising edge, moving towards the trough, and ending at the falling edge.\n",
    "\n",
    "We can exemplify this pattern by plotting the spike times aligned to the phase of theta. We'll want the corresponding phase of theta at which the unit fires as the animal is running down the track, which we can again compute using the method [`value_from`](https://pynapple.org/generated/pynapple.TsGroup.value_from.html). \n",
    "\n",
    "\n",
    "\n",
    "#### 14. Compute the value of `theta_phase` corresponding to each spike time from unit 177."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04bda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_phase = spikes[unit].value_from(theta_phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a542dc",
   "metadata": {},
   "source": [
    "To visualize the results, we'll recreate the plot above, but instead with the theta phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b556071",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(3, 1, figsize=(10,6), constrained_layout=True, sharex=True)\n",
    "axs[0].plot(theta_band.restrict(ex_run_ep), color=\"slateblue\", label=\"filtered theta\")\n",
    "axs[0].plot(spike_theta.restrict(ex_run_ep), 'o', color=\"orange\", label=\"spike times\")\n",
    "axs[0].set(ylabel=\"LFP (a.u.)\", title=\"Spike times relative to filtered theta\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(theta_phase.restrict(ex_run_ep), color=\"slateblue\", label=\"theta phase\")\n",
    "axs[1].plot(spike_phase.restrict(ex_run_ep), 'o', color=\"orange\", label=\"spike times\")\n",
    "axs[1].set(ylabel=\"Phase (rad)\", title=\"Spike times relative to theta phase\")\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(ex_position, '--', color=\"green\", label=\"animal position\")\n",
    "axs[2].plot(ex_position[(ex_position > 50).values & (ex_position < 130).values], color=\"green\", lw=3, label=\"place field bounds\")\n",
    "axs[2].set(ylabel=\"Position (cm)\", xlabel=\"Time (s)\", title=\"Animal position\")\n",
    "axs[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5de85",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-07.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e428a1e",
   "metadata": {},
   "source": [
    "    \n",
    "We now see a negative trend in the spike phase as the animal moves through unit 177's place field. This phemomena is known as phase precession: the phase at which a unit spikes *precesses* (gets earlier) as the animal runs through that unit's place field. Explicitly, that unit will spike at *late* phases of theta (higher radians) in *earlier* positions in the field, and fire at *early* phases of theta (lower radians) in *late* positions in the field.\n",
    "\n",
    "We can observe this phenomena on average across the session by relating the spike phase to the spike position. \n",
    "\n",
    "\n",
    "\n",
    "#### 15. Compute the position corresponding to each spike for example unit 177."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ef032",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_position = spikes[unit].value_from(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f27a0",
   "metadata": {},
   "source": [
    "Now we can plot the spike phase against the spike position in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b515db",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(5,3))\n",
    "axs.plot(spike_position, spike_phase, 'o')\n",
    "axs.set_ylabel(\"Phase (rad)\")\n",
    "axs.set_xlabel(\"Position (cm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d84464",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-08.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d12b26d",
   "metadata": {},
   "source": [
    "    \n",
    "Similar to what we saw in a single run, there is a negative relationship between theta phase and field position, characteristic of phase precession.\n",
    "\n",
    "\n",
    "\n",
    "### Computing 2D tuning curves: position vs. phase\n",
    "\n",
    "\n",
    "\n",
    "The scatter plot above can be similarly be represented as a 2D tuning curve over position and phase. We can compute this using the same function, [`nap.compute_tuning_curves`](https://pynapple.org/generated/pynapple.process.tuning_curves.html#pynapple.process.tuning_curves.compute_tuning_curves), but now passing second input, `features`, as a 2-column `TsdFrame` containing the two target features.\n",
    "\n",
    "To do this, we'll need to combine `position` and `theta_phase` into a `TsdFrame`. For this to work, both variables must have the same length. We can achieve this by upsampling `position` to the length of `theta_phase` using the pynapple object method [`interpolate`](https://pynapple.org/generated/pynapple.Tsd.interpolate.html). This method will linearly interpolate new position samples between existing position samples at timestamps given by another pynapple object, in our case by `theta_phase`. Once they're the same length, they can be combined into a single `TsdFrame` and used to compute 2D tuning curves.\n",
    "\n",
    "\n",
    "\n",
    "#### 16. Interpolate `position` to the time points of `theta_phase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_pos = position.interpolate(theta_phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2659b01",
   "metadata": {},
   "source": [
    "Let's visualize the results of the interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f9125",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,1,constrained_layout=True,sharex=True,figsize=(10,4))\n",
    "axs[0].plot(position.restrict(ex_run_ep),'.')\n",
    "axs[0].set(ylabel=\"Position (cm)\", title=\"Original position points\")\n",
    "axs[1].plot(upsampled_pos.restrict(ex_run_ep),'.')\n",
    "axs[1].set(ylabel=\"Position (cm)\", xlabel=\"Time (s)\", title=\"Upsampled position points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f890f5",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-09.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eab685",
   "metadata": {},
   "source": [
    "#### 17. Stack `upsampled_pos` and `theta_phase` together into a single [`TsdFrame`](https://pynapple.org/generated/pynapple.TsdFrame.html)\n",
    "\n",
    "\n",
    "    \n",
    "- Make sure to name your `TsdFrame` columns `\"position\"` and `\"phase\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.stack((upsampled_pos.values, theta_phase.values))\n",
    "features = nap.TsdFrame(\n",
    "    t=theta_phase.t,\n",
    "    d=np.transpose(feats),\n",
    "    time_support=upsampled_pos.time_support,\n",
    "    columns=[\"position\", \"phase\"],\n",
    ")\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150c27f",
   "metadata": {},
   "source": [
    "#### 18. Apply [`nap.compute_tuning_curves`](https://pynapple.org/generated/pynapple.process.tuning_curves.html#pynapple.process.tuning_curves.compute_tuning_curves) with `features` on our subselected group of units, `good_spikes`\n",
    "\n",
    "\n",
    "\n",
    "- Use 50 bins for position and 20 bins for theta phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_curves = nap.compute_tuning_curves(good_spikes, features, bins=[50,20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3515b31",
   "metadata": {},
   "source": [
    "We can plot 2D tuning curves for each unit and visualize how many of these units are phase precessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903865d1",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "tc_norm = tuning_curves / tuning_curves.max(axis=(1,2))\n",
    "p = tc_norm.plot(x=\"position\", y=\"phase\", col=\"unit\", col_wrap=5, size=1.2, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeaf6cf",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p.fig.savefig(\"../../_static/_check_figs/02-10.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0edd8",
   "metadata": {},
   "source": [
    "We can see a number of units that display a negative relationship between position and phase, characteristic of phase precession. In addition to 177, units 23, 33, and 146 are good examples of this phenomena. In contrast, units 125 and 258 don't appear to be phase precessing.\n",
    "\n",
    "\n",
    "\n",
    "### Decoding position from spiking activity\n",
    "\n",
    "<div class=\"render-all\">\n",
    "\n",
    "Next we'll do a popular analysis in the rat hippocampus sphere: Bayesian decoding. This analysis is an elegent application of Bayes' rule in predicting the animal's location (or other behavioral variables) given neural activity at some point in time. Refer to the dropdown box below for a more in-depth explanation.\n",
    "\n",
    ":::{admonition} Background: Bayesian decoding\n",
    ":class: dropdown\n",
    "Recall Bayes' rule, written here in terms of our relevant variables:\n",
    "\n",
    "$$P(position|spikes) = \\frac{P(position)P(spikes|position)}{P(spikes)}$$\n",
    "\n",
    "Our goal is to compute the unknown posterior $P(position|spikes)$ given known prior $P(position)$ and known likelihood $P(spikes|position)$. \n",
    "\n",
    "$P(position)$, also known as the *occupancy*, is the probability that the animal is occupying some position. This can be computed exactly by the proportion of the total time spent at each position, but in many cases it is sufficient to estimate the occupancy as a uniform distribution, i.e. it is equally likely for the animal to occupy any location.\n",
    "\n",
    "The next term, $P(spikes|position)$, which is the probability of seeing some sequence of spikes across all neurons at some position. Computing this relys on the following assumptions:\n",
    "1. Neurons fire according to a Poisson process (i.e. their spiking activity follows a Poisson distribution)\n",
    "2. Neurons fire independently from one another.\n",
    "\n",
    "While neither of these assumptions are strictly true, they are generally reasonable for pyramidal cells in hippocampus and allow us to simplify our computation of $P(spikes|position)$\n",
    "\n",
    "The first assumption gives us an equation for $P(spikes|position)$ for a single neuron, which we'll call $P(spikes_i|position)$ to differentiate it from $P(spikes|position) = P(spikes_1,spikes_2,...,spikes_N|position)$, or the total probability across all $N$ neurons. The equation we get is that of the Poisson distribution:\n",
    "\n",
    "$$\n",
    "P(spikes_i|position) = \\frac{(\\tau f_i(position))^n e^{-\\tau f_i(position)}}{n!}\n",
    "$$\n",
    "\n",
    "where $f_i(position)$ is the firing rate of the neuron at position $(position)$ (i.e. the tuning curve), $\\tau$ is the width of the time window over which we're computing the probability, and $n$ is the total number of times the neuron spiked in the time window of interest.\n",
    "\n",
    "The second assumptions allows us to simply combine the probabilities of individual neurons. Recall the product rule for independent events: $P(A,B) = P(A)P(B)$ if $A$ and $B$ are independent. Treating neurons as independent, then, gives us the following:\n",
    "\n",
    "$$\n",
    "P(spikes|position) = \\prod_i P(spikes_i|position)\n",
    "$$\n",
    "\n",
    "The final term, $P(spikes)$, is inferred indirectly using the law of total probability:\n",
    "\n",
    "$$P(spikes) = \\sum_{position}P(position,spikes) = \\sum_{position}P(position)P(spikes|position)$$\n",
    "\n",
    "Another way of putting it is $P(spikes)$ is the normalization factor such that $\\sum_{position} P(position|spikes) = 1$, which is achived by dividing the numerator by its sum.\n",
    "\n",
    "If this method looks daunting, we have some good news: pynapple has it implemented already in the function `nap.decode_bayes`. All we'll need are the spikes, the tuning curves, and the width of the time window $\\tau$.\n",
    ":::\n",
    "\n",
    "(phase-precess-cv-presenters)=\n",
    ":::{admonition} Aside: Cross-validation\n",
    ":class: tip \n",
    "    \n",
    "Generally this method is cross-validated, which means you train the model on one set of data and test the model on a different, held-out data set. For Bayesian decoding, the \"model\" refers to the model *likelihood*, which is computed from the tuning curves. \n",
    "\n",
    "If we want to decode an example run down the track, our training set should omit this run before computing the tuning curves. We can do this by using the IntervalSet method `set_diff`, to take out the example run epoch from all run epochs. Next, we'll restrict our data to these training epochs and re-compute the place fields using `nap.compute_tuning_curves`. We'll also apply a Gaussian smoothing filter to the place fields, which will smooth our decoding results down the line.\n",
    "\n",
    "The code cell below will do these steps for you.\n",
    ":::\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ae289",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "# hold out trial from place field computation\n",
    "run_train = forward_ep.set_diff(ex_run_ep)\n",
    "# get position of training set\n",
    "position_train = position.restrict(run_train)\n",
    "# compute place fields using training set\n",
    "place_fields = nap.compute_tuning_curves(spikes, position_train, bins=50, feature_names=[\"position\"])\n",
    "# smooth place fields\n",
    "place_fields.data = gaussian_filter1d(place_fields.data, 1, axis=-1)\n",
    "\n",
    "# plot sorted, normalized tuning curves\n",
    "idx = place_fields.argmax(axis=1)\n",
    "place_fields_sorted = place_fields.sortby(idx)\n",
    "place_fields_sorted[\"unit\"] = np.arange(place_fields_sorted.shape[0])\n",
    "p = (place_fields_sorted / place_fields_sorted.max(axis=1)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043a4f9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p.figure.savefig(\"../../_static/_check_figs/02-11.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce1bc63",
   "metadata": {},
   "source": [
    "We can decode any number of features using the function [`nap.decode_bayes`](https://pynapple.org/generated/pynapple.process.decoding.html#pynapple.process.decoding.decode_bayes), which will decode any number of features given by the input `tuning_curves`, computed by [`nap.compute_tuning_curves`](https://pynapple.org/generated/pynapple.process.tuning_curves.html#pynapple.process.tuning_curves.compute_tuning_curves).\n",
    "\n",
    "#### 19. Use [`nap.decode_bayes`](https://pynapple.org/generated/pynapple.process.decoding.html#pynapple.process.decoding.decode_bayes) to decode position during `ex_run_ep`\n",
    "\n",
    "- Use 40 ms time bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_position, decoded_prob = nap.decode_bayes(place_fields, spikes, ex_run_ep, 0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042c914",
   "metadata": {},
   "source": [
    "Let's plot decoded position with the animal's true position. We'll overlay them on a heat map of the decoded probability to visualize the confidence of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61b2db",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10, 4), constrained_layout=True)\n",
    "c = ax.pcolormesh(decoded_position.index,place_fields.position,np.transpose(decoded_prob))\n",
    "ax.plot(decoded_position, \"--\", color=\"red\", label=\"decoded position\")\n",
    "ax.plot(ex_position, color=\"red\", label=\"true position\")\n",
    "ax.legend()\n",
    "fig.colorbar(c, label=\"decoded probability\")\n",
    "ax.set(xlabel=\"Time (s)\", ylabel=\"Position (cm)\", );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc2e0d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-12.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a2067",
   "metadata": {},
   "source": [
    "    \n",
    "While the decoder generally follows the animal's true position, there is still a lot of error in the decoder, especially later in the run. We can improve the decoder error by smoothing the spike counts. [`nap.decode_bayes`](https://pynapple.org/generated/pynapple.process.decoding.html#pynapple.process.decoding.decode_bayes) provides the option to do this for you by specifying `sliding_window_size`, which specifies the width, in number of bins, of a uniform (all ones) kernel to convolve with the spike counts. This is equivalent to applying a moving sum to adjacent bins, where the width of the kernel is the number of adjacent bins being added together. This is equivalent to counting spikes in a *sliding window* that shifts in shorter increments than the window's width, resulting in bins that overlap. This combines the accuracy of using a wider time bin with the temporal resolution of a shorter time bin.\n",
    "\n",
    "For example, let's say we want a sliding window of $200 ms$ that shifts by $40 ms$. This is equivalent to summing together 5 adjacent $40 ms$ bins, or convolving spike counts in $40 ms$ bins with a length-5 array of ones ($[1, 1, 1, 1, 1]$). Let's visualize this convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c300b74",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "ex_counts = spikes[unit].restrict(ex_run_ep).count(0.04)\n",
    "workshop_utils.animate_1d_convolution(ex_counts, np.ones(5), tsd_label=\"original counts\", kernel_label=\"moving sum\", conv_label=\"convolved counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e0095",
   "metadata": {},
   "source": [
    "    \n",
    "The count at each time point is computed by convolving the kernel (yellow), centered at that time point, with the original spike counts (blue). For a length-5 kernel of ones, this amounts to summing the counts in the center bin with two bins before and two bins after (shaded green, top). The result is an array of counts smoothed out in time (green, bottom).\n",
    "\n",
    "\n",
    "\n",
    "#### 20. Decode the same run as above, now using sliding window size of 5 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dcf70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smth_decoded_position, smth_decoded_prob = nap.decode_bayes(place_fields, spikes, ex_run_ep, bin_size=0.04, sliding_window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36a53c",
   "metadata": {},
   "source": [
    "Let's plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c0dc5",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10, 4), constrained_layout=True)\n",
    "c = ax.pcolormesh(smth_decoded_position.index,place_fields.position,np.transpose(smth_decoded_prob))\n",
    "ax.plot(smth_decoded_position, \"--\", color=\"red\", label=\"decoded position\")\n",
    "ax.plot(ex_position, color=\"red\", label=\"true position\")\n",
    "ax.legend()\n",
    "fig.colorbar(c, label=\"decoded probability\")\n",
    "ax.set(xlabel=\"Time (s)\", ylabel=\"Position (cm)\", );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ea660",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-13.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ae017",
   "metadata": {},
   "source": [
    "    \n",
    "This gives us a much closer approximation of the animal's true position.\n",
    "\n",
    "Units phase precessing together creates fast, spatial sequences around the animal's true position. We can reveal this by decoding at an even shorter time scale, which will appear as smooth errors in the decoder.\n",
    "\n",
    "\n",
    "\n",
    "#### 21. Decode again using a smaller bin size of $10 ms$ and sliding window size of 5 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "smth_decoded_position, smth_decoded_prob = nap.decode_bayes(place_fields, spikes, ex_run_ep, bin_size=0.01, sliding_window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abec8f0",
   "metadata": {},
   "source": [
    "    \n",
    "We'll make the same plot as before to visualize the results, but plot it alongside the raw and filtered LFP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d397c03",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10, 4), constrained_layout=True, height_ratios=[3,1], sharex=True)\n",
    "c = axs[0].pcolormesh(smth_decoded_prob.index, smth_decoded_prob.columns, np.transpose(smth_decoded_prob))\n",
    "p1 = axs[0].plot(smth_decoded_position, \"--\", color=\"r\")\n",
    "p2 = axs[0].plot(ex_position, color=\"r\")\n",
    "axs[0].set_ylabel(\"Position (cm)\")\n",
    "axs[0].legend([p1[0],p2[0]],[\"decoded position\",\"true position\"])\n",
    "fig.colorbar(c, label = \"predicted probability\")\n",
    "\n",
    "axs[1].plot(lfp.restrict(ex_run_ep))\n",
    "axs[1].plot(theta_band.restrict(ex_run_ep))\n",
    "axs[1].set_ylabel(\"LFP (a.u.)\")\n",
    "\n",
    "fig.supxlabel(\"Time (s)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a23d46",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-14.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe9fb0",
   "metadata": {},
   "source": [
    "    \n",
    "The estimated position oscillates with cycles of theta, where each \"sweep\" is referred to as a \"theta sequence\". Fully understanding the properties of theta sequences and their role in learning, memory, and planning is an active topic of research in Neuroscience!\n",
    "\n",
    "\n",
    "\n",
    "### Bonus Exercise\n",
    "\n",
    "\n",
    "    \n",
    "Pynapple has another decoding method, [`nap.decode_template`](https://pynapple.org/generated/pynapple.process.decoding.html#pynapple.process.decoding.decode_template), that is agnostic to the underlying noise model of the data. In other words, where the above implementation of Bayesian decoding is specific to spiking data (Poisson distributed data), template decoding can be applied to any data modality. As a bonus exercise, you can try decoding position using this method and compare the results to the Bayesian decoder used above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5fc094",
   "metadata": {},
   "source": [
    "## Part 2: Using NeMoS to disentangle position and speed encoding\n",
    "\n",
    "\n",
    "    \n",
    "Up until now, we have been primarily studying how *position* influences hippocampal firing. How can we be confident that position is influencing the firing rate and not other, correlated variables? This can be disentangled by fitting a GLM.\n",
    "\n",
    "\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "\n",
    "    \n",
    "In case any variables got lost or overwritten during part 1, we'll redefine everything we need for part 2 in the cell below. \n",
    "\n",
    "To decrease computation time, we're going to spend the rest of the notebook focusing on three selected neurons. For GLM fitting, we're going to bin spikes at 100 Hz and up-sample the position to match that temporal resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf6365",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "forward_ep = data[\"forward_ep\"]\n",
    "position = data[\"position\"].restrict(forward_ep)\n",
    "spikes = data[\"units\"]\n",
    "place_fields = nap.compute_tuning_curves(spikes, position, bins=50, epochs=position.time_support, feature_names=[\"distance\"])\n",
    "\n",
    "neurons = [82, 92, 220]\n",
    "place_fields = place_fields.sel(unit=neurons)\n",
    "spikes = spikes[neurons]\n",
    "bin_size = .01\n",
    "count = spikes.count(bin_size, ep=position.time_support)\n",
    "position = position.interpolate(count, ep=count.time_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df1499",
   "metadata": {},
   "source": [
    "### Visualizing speed\n",
    "\n",
    "\n",
    "\n",
    "One competing variable is speed: the speed at which the animal traverse the field is not homogeneous. Does it influence the firing rate of hippocampal neurons? We can compute tuning curves for speed as well as average speed across the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8967ee",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "speed = []\n",
    "# Analyzing each epoch separately avoids edge effects.\n",
    "for s, e in position.time_support.values: \n",
    "    pos_ep = position.get(s, e)\n",
    "    # Absolute difference of two consecutive points\n",
    "    speed_ep = np.abs(np.diff(pos_ep)) \n",
    "    # Padding the edge so that the size is the same as the position/spike counts\n",
    "    speed_ep = np.pad(speed_ep, [0, 1], mode=\"edge\") \n",
    "    # Converting to cm/s \n",
    "    speed_ep = speed_ep * position.rate\n",
    "    speed.append(speed_ep)\n",
    "\n",
    "speed = nap.Tsd(t=position.t, d=np.hstack(speed), time_support=position.time_support)\n",
    "print(speed.shape)\n",
    "\n",
    "tc_speed = nap.compute_tuning_curves(spikes, speed, bins=20, epochs=speed.time_support, feature_names=[\"speed\"])\n",
    "fig = workshop_utils.plot_position_speed(position, speed, place_fields.sel(unit=neurons), tc_speed, neurons);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7bef5e",
   "metadata": {},
   "source": [
    "(basis-eval-place-cells-presenters)=\n",
    "### Basis evaluation\n",
    "\n",
    ":::{note}\n",
    ":class: render-all\n",
    "\n",
    "This afternoon, we'll show how to cross-validate across basis identity, which you can use to choose the basis.\n",
    "\n",
    ":::\n",
    "\n",
    "#### 1. Instantiate the basis by doing the following:\n",
    "\n",
    "\n",
    "\n",
    "- Create a separate basis object for each model input (speed and position).\n",
    "- Use `BSplineEval` basis with 10 basis functions each.\n",
    "- Provide a label for each basis (\"position\" and \"speed\").\n",
    "- Visualize the basis objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_basis = nmo.basis.BSplineEval(n_basis_funcs=10, label=\"position\")\n",
    "speed_basis = nmo.basis.BSplineEval(n_basis_funcs=10, label=\"speed\")\n",
    "fig = workshop_utils.plot_pos_speed_bases(position_basis, speed_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f588d4",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-15.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba958d91",
   "metadata": {},
   "source": [
    "    \n",
    "However, now we have an issue: in all our previous examples, we had a single basis object, which took a single input to produce a single array which we then passed to the `GLM` object as the design matrix. What do we do when we have multiple basis objects?\n",
    "\n",
    "For people new to NeMoS, but familiar with NumPy, you can call `basis.compute_features()` for each basis separately and then [concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) the outputs.\n",
    "\n",
    "For people familiar with [NeMoS basis composition](https://nemos.readthedocs.io/en/latest/background/basis/plot_02_ND_basis_function.html), you can add the two bases together obtaining a new 2D basis, then call `compute_features` passing both position and speed to obtain the same design matrix.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Create a design matrix by doing one of the following:\n",
    "\n",
    "\n",
    "\n",
    "2.1. Call `compute_fatures` for both position and speed bases and concatenate the result to form a single design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent to calling nmo.basis.AdditiveBasis(position_basis, speed_basis)\n",
    "X_position = position_basis.compute_features(position)\n",
    "X_speed = speed_basis.compute_features(speed)\n",
    "X = np.concatenate([X_position, X_speed], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09946d9e",
   "metadata": {},
   "source": [
    "2.2. Add the basis objects together and call `compute_fatures` on the newly created additive basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent to calling nmo.basis.AdditiveBasis(position_basis, speed_basis)\n",
    "basis = position_basis + speed_basis\n",
    "X = basis.compute_features(position, speed)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b7f1c",
   "metadata": {},
   "source": [
    "Notice that, since we passed pynapple objects to the basis, we got a pynapple object back, preserving the time stamps. Additionally, `X` has the same number of time points as our input position and speed, but 20 columns. The columns come from  `n_basis_funcs` from each basis (10 for position, 10 for speed).\n",
    "\n",
    "\n",
    "\n",
    "### Model learning\n",
    "\n",
    "\n",
    "\n",
    "As we've done before, we can now use the Poisson GLM from NeMoS to learn the combined model.\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Fit a GLM by doing the following:\n",
    "\n",
    "\n",
    "\n",
    "- Initialize `PopulationGLM`\n",
    "- Use the \"LBFGS\" solver and pass `{\"tol\": 1e-12}` to `solver_kwargs`.\n",
    "- Fit the data, passing the design matrix and spike counts to the glm object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dfad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm = nmo.glm.PopulationGLM(\n",
    "    solver_kwargs={\"tol\": 1e-12},\n",
    "    solver_name=\"LBFGS\",\n",
    ")\n",
    "\n",
    "glm.fit(X, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e40f022",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "\n",
    "\n",
    "Let's check first if our model can accurately predict the tuning curves we displayed above. We can use the [`predict`](https://nemos.readthedocs.io/en/latest/generated/glm/nemos.glm.GLM.predict.html#nemos.glm.GLM.predict) function of NeMoS and then compute new tuning curves. Set `bins=50` for position and `bins=30` for speed.\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Use `predict` to check whether our GLM has captured each neuron's speed and position tuning.\n",
    "\n",
    "\n",
    "\n",
    "- Remember to convert the predicted firing rate to spikes per second!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad4734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the model's firing rate\n",
    "predicted_rate = glm.predict(X) / bin_size\n",
    "\n",
    "# same shape as the counts we were trying to predict\n",
    "print(predicted_rate.shape, count.shape)\n",
    "\n",
    "# compute the position and speed tuning curves using the predicted firing rate.\n",
    "glm_tuning_pos = nap.compute_tuning_curves(predicted_rate, position, bins=50, epochs=position.time_support, feature_names=[\"position\"])\n",
    "glm_tuning_speed = nap.compute_tuning_curves(predicted_rate, speed, bins=30, epochs=speed.time_support, feature_names=[\"speed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e16920",
   "metadata": {},
   "source": [
    "We can plot the results to compare the model and data tuning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053ea4f",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "fig = workshop_utils.plot_position_speed_tuning(place_fields, tc_speed, glm_tuning_pos, glm_tuning_speed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d1726",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"../../_static/_check_figs/02-16.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6260a82",
   "metadata": {},
   "source": [
    "We can see that this model does a good job capturing both the position and the speed. \n",
    "\n",
    "\n",
    "\n",
    "### Bonus Exercise\n",
    "\n",
    "\n",
    "\n",
    "As an bonus, more open-ended exercise, we can investigate all the scientific decisions that we swept under the rug: should we regularize the model? What basis should we use? Do we need both inputs? If you're feeling ambitious, here are some suggestions to answer these questions:\n",
    "\n",
    "- Try to fit and compare the results we just obtained with different models: \n",
    "  - A model with position as the only predictor.\n",
    "  - A model with speed as the only predictor.\n",
    "- Introduce L1 (Lasso) regularization and fit models with increasingly large penalty strengths ($\\lambda$). Plot the regularization path showing how each coefficient changes with $\\lambda$. Identify which coefficients remain non-zero longest as $\\lambda$ increases - these correspond to the most informative predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beba2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2d51e",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "\n",
    "The data in this tutorial comes from [Grosmark, Andres D., and Gyrgy Buzski. \"Diversity in neural firing dynamics supports both rigid and learned hippocampal sequences.\" Science 351.6280 (2016): 1440-1443](https://www.science.org/doi/full/10.1126/science.aad1935)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.18.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "no-search": true,
  "orphan": true,
  "source_map": [
   16,
   29,
   68,
   91,
   107,
   115,
   138,
   142,
   150,
   154,
   164,
   170,
   176,
   180,
   190,
   200,
   210,
   214,
   222,
   226,
   236,
   240,
   250,
   254,
   262,
   271,
   281,
   288,
   300,
   303,
   313,
   316,
   332,
   337,
   347,
   350,
   362,
   366,
   374,
   391,
   395,
   417,
   420,
   436,
   439,
   452,
   473,
   477,
   495,
   499,
   515,
   517,
   525,
   537,
   541,
   555,
   560,
   568,
   586,
   590,
   610,
   612,
   623,
   625,
   645,
   657,
   661,
   679,
   682,
   690,
   706,
   710,
   722,
   724,
   732,
   752,
   756,
   768,
   770,
   778,
   787,
   791,
   811,
   813,
   821,
   831,
   835,
   845,
   854,
   864,
   866,
   874,
   881,
   885,
   951,
   970,
   974,
   986,
   988,
   996,
   1008,
   1012,
   1022,
   1027,
   1037,
   1039,
   1047,
   1059,
   1063,
   1075,
   1077,
   1085,
   1103,
   1107,
   1123,
   1125,
   1145,
   1159,
   1169,
   1189,
   1212,
   1218,
   1222,
   1242,
   1248,
   1256,
   1261,
   1287,
   1294,
   1312,
   1322,
   1330,
   1336,
   1340,
   1361,
   1363
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}