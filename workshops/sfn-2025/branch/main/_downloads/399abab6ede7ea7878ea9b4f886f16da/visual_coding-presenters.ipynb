{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f3c7e",
   "metadata": {
    "tags": [
     "hide-input",
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"plotting functions contained within `_documentation_utils` are intended for nemos's documentation.\",\n",
    "    category=UserWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a69cd",
   "metadata": {},
   "source": [
    ":::{admonition} Download\n",
    ":class: important render-all\n",
    "\n",
    "This notebook can be downloaded as **{nb-download}`visual_coding-presenters.ipynb`**. See the button at the top right to download as markdown or pdf.\n",
    ":::\n",
    "\n",
    "# Exploring the Visual Coding Dataset\n",
    "This notebook has had all its explanatory text removed and has not been run.\n",
    " It is intended to be downloaded and run locally (or on the provided binder)\n",
    " while listening to the presenter's explanation. In order to see the fully\n",
    " rendered of this notebook, go [here](../../full/day2/visual_coding.md)\n",
    "\n",
    "\n",
    "\n",
    "This notebook serves as a group project: in groups of 4 or 5, you will analyze data from the [Visual Coding - Neuropixels dataset](https://portal.brain-map.org/circuits-behavior/visual-coding-neuropixels), published by the Allen Institute. This dataset uses [extracellular electrophysiology probes](https://www.nature.com/articles/nature24636) to record spikes from multiple regions in the brain during passive visual stimulation.\n",
    "\n",
    "To start, we will focus on the activity of neurons in the visual cortex (VISp) during passive exposure to full-field flashes of color either black (coded as \"-1.0\") or white (coded as \"1.0\") in a gray background. If you have time, you can apply the same procedure to other stimuli or brain areas.\n",
    "\n",
    "For this exercise, you will:\n",
    "- Compute Peristimulus Time Histograms (PSTHs) and select relevant neurons to analyze using `pynapple`.\n",
    "- Fit GLMs to these neurons using `nemos`.\n",
    "\n",
    "As this is the last notebook, the instructions are a bit more hands-off: you will make more of the analysis and modeling decisions yourselves. As a group, you will use your neuroscience knowledge and the skills gained over this workshop to decide:\n",
    "- How to select relevant neurons.\n",
    "- How to avoid overfitting.\n",
    "- What features to include in your GLMs.\n",
    "- Which basis functions (and parameters) to use for each feature.\n",
    "- How to regularize your features.\n",
    "- How to evaluate your model.\n",
    "\n",
    "At the end of this session, we will regroup to discuss the decisions people made and evaluate each others' models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d37e3e",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "# Import everything\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pynapple as nap\n",
    "\n",
    "import nemos as nmo\n",
    "\n",
    "# some helper plotting functions\n",
    "from nemos import _documentation_utils as doc_plots\n",
    "import workshop_utils\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# configure plots some\n",
    "plt.style.use(nmo.styles.plot_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89094d",
   "metadata": {},
   "source": [
    "## Downloading and preparing data\n",
    "\n",
    "\n",
    "In this section, we will download the data from DANDI and extract the relevant parts for analysis and modeling. This section is largely presented to you as is, so that you can get to the substantive sections more quickly.\n",
    "\n",
    "First we download and load the data into pynapple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470dc74",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "dandiset_id = \"000021\"\n",
    "dandi_filepath = \"sub-726298249/sub-726298249_ses-754829445.nwb\"\n",
    "\n",
    "# Download the data using NeMoS\n",
    "io = nmo.fetch.download_dandi_data(dandiset_id, dandi_filepath)\n",
    "\n",
    "# load data using pynapple\n",
    "data = nap.NWBFile(io.read(), lazy_loading=True)\n",
    "\n",
    "# grab the spiking data\n",
    "units = data[\"units\"]\n",
    "\n",
    "# map from electrodes to brain area\n",
    "channel_probes = {}\n",
    "for elec in data.nwb.electrodes:\n",
    "    channel_id = elec.index[0]\n",
    "    location = elec[\"location\"].values[0]\n",
    "    channel_probes[channel_id] = location\n",
    "\n",
    "# Add a new column to include location in our spikes TsGroup\n",
    "units.brain_area = [channel_probes[int(ch_id)] for ch_id in units.peak_channel_id]\n",
    "\n",
    "# drop unnecessary metadata\n",
    "units.restrict_info([\"rate\", \"quality\", \"brain_area\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b0892f",
   "metadata": {},
   "source": [
    "Now that we have our spiking data, let's restrict our dataset to the relevant part.\n",
    "\n",
    "![Visual stimuli set](../../_static/visual_stimuli_set.png)\n",
    "\n",
    "During the flashes presentation trials, mice were exposed to white or black full-field flashes in a gray background, each lasting 250 ms, and separated by a 2 second inter-trial interval. In total, they were exposed to 150 flashes (75 black, 75 white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64817a6a",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "flashes = data[\"flashes_presentations\"]\n",
    "flashes.restrict_info([\"color\"])\n",
    "\n",
    "# create a separate object for black and white flashes\n",
    "flashes_white = flashes[flashes[\"color\"] == \"1.0\"]\n",
    "flashes_black = flashes[flashes[\"color\"] == \"-1.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a9bf4e",
   "metadata": {},
   "source": [
    "<div class=\"render-all\">\n",
    "\n",
    ":::{admonition} Other stimulus classes?\n",
    ":class: dropdown\n",
    "\n",
    "As can be seen in the image above, there are many other stimulus types shown in this experiment. If you finish analyzing the flashes and want to look at one of the others, you can do so by grabbing the corresponding `IntervalSet` from the `data` object; they are all named `{stim_type}_presentations`, where `{stim_type}` may be `flashes`, `gabors`, `static_gratings`, etc.\n",
    "\n",
    ":::\n",
    "\n",
    "Let's visualize our stimuli:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13792faa",
   "metadata": {
    "tags": [
     "render-all",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "n_flashes = 5\n",
    "n_seconds = 13\n",
    "offset = .5\n",
    "\n",
    "start = data[\"flashes_presentations\"][\"start\"].min() - offset\n",
    "end = start + n_seconds\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (17, 4))\n",
    "for flash, c in zip([flashes_white, flashes_black], [\"silver\", \"black\"]):\n",
    "    for fl in flash[:n_flashes]:\n",
    "        ax.axvspan(fl.start[0], fl.end[0], color=c, alpha=.4, ec=c)      \n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Absent = 0, Present = 1\")\n",
    "ax.set_title(\"Stimuli presentation\")\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.xlim(start-.1,end)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba36416e",
   "metadata": {},
   "source": [
    "## Preliminary analyses and neuron selection\n",
    "\n",
    "\n",
    "\n",
    "From here on out, you will write the code yourself. This first section will involve us doing some preliminary analyses to find the neurons that are most visually responsive; these are the neurons we will fit our GLM to.\n",
    "\n",
    "First, let's construct a {class}`~pynapple.IntervalSet` called `extended_flashes` which contains the peristimulus time. Right now, our `flashes` `IntervalSet` defines the start and end time for the flashes. In order to make sure we can model the pre-stimulus baseline and any responses to the stimulus being turned off, we would like to expand these intervals to go from 500 msecs before the start of the stimuli to 500 msecs after the end.\n",
    "\n",
    "This `IntervalSet` should be the same shape as `flashes` and have the same metadata columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81236db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = .50 # 500 ms\n",
    "start = flashes.start - dt # Start 500 ms before stimulus presentation\n",
    "end = flashes.end + dt # End 500 ms after stimulus presentation\n",
    "\n",
    "extended_flashes = nap.IntervalSet(start, end, metadata=flashes.metadata) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af845222",
   "metadata": {},
   "source": [
    "If you have succeeded, the following should pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c035b1",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "assert extended_flashes.shape == flashes.shape\n",
    "assert all(extended_flashes.metadata == flashes.metadata)\n",
    "assert all(extended_flashes.start == flashes.start - .5)\n",
    "assert all(extended_flashes.end == flashes.end + .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4374f9c",
   "metadata": {},
   "source": [
    "Now, create two separate `IntervalSet` objects, `extended_flashes_black` and `extended_flashes_white`, which contain this info for only the black and the white flashes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134822d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_flashes_white = extended_flashes[extended_flashes[\"color\"] == \"1.0\"]\n",
    "extended_flashes_black = extended_flashes[extended_flashes[\"color\"] == \"-1.0\"]\n",
    "\n",
    "# OR\n",
    "\n",
    "dt = .50 # 500 ms\n",
    "start = flashes_white.start - dt # Start 500 ms before stimulus presentation\n",
    "end = flashes_white.end + dt # End 500 ms after stimulus presentation\n",
    "extended_flashes_white = nap.IntervalSet(start, end, metadata=flashes_white.metadata) \n",
    "\n",
    "start = flashes_black.start - dt # Start 500 ms before stimulus presentation\n",
    "end = flashes_black.end + dt # End 500 ms after stimulus presentation\n",
    "extended_flashes_black = nap.IntervalSet(start, end, metadata=flashes_black.metadata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce5fdb",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "# This should all pass if you created the IntervalSet correctly\n",
    "assert extended_flashes_white.shape == flashes_white.shape\n",
    "assert all(extended_flashes_white.metadata == flashes_white.metadata)\n",
    "assert all(extended_flashes_white.start == flashes_white.start - .5)\n",
    "assert all(extended_flashes_white.end == flashes_white.end + .5)\n",
    "assert extended_flashes_black.shape == flashes_black.shape\n",
    "assert all(extended_flashes_black.metadata == flashes_black.metadata)\n",
    "assert all(extended_flashes_black.start == flashes_black.start - .5)\n",
    "assert all(extended_flashes_black.end == flashes_black.end + .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dd3e1f",
   "metadata": {},
   "source": [
    "<div class=\"render-all\">\n",
    "\n",
    "Now, select your neurons. There are four criteria we want to use:\n",
    "\n",
    "1. Brain area: we are interested in analyzing VISp units for this tutorial\n",
    "2. Quality: we will only select “good” quality units. If you're curious, you can (optionally) [read more](https://alleninstitute.github.io/openscope_databook/visualization/visualize_unit_metrics.html) how about the Allen Institute defines quality.\n",
    "3. Firing rate: overall, we want units with a firing rate larger than 2Hz around the presentation of stimuli\n",
    "4. Responsiveness: we want units that actually respond to changes in the visual stimuli, i.e., their firing rate changes as a result of the stimulus.\n",
    "\n",
    "Create a new `TsGroup`, `selected_units`, which includes only those units that meet the first three criteria, then check that it passes the assertion block.\n",
    "\n",
    ":::{admonition} Restrict!\n",
    ":class: reminder\n",
    "\n",
    "Don't forget when selecting based on firing rate that we want neurons whose firing rate is above the threshold **around the presentation of the stimuli!** This means you should use {func}`~pynapple.TsGroup.restrict`! If only we had a useful `IntervalSet` lying around...\n",
    "\n",
    ":::\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2dc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter units according criteria 1 & 2\n",
    "selected_units = units[(units[\"brain_area\"]==\"VISp\") & (units[\"quality\"]==\"good\")] \n",
    "\n",
    "# Restrict around stimuli presentation\n",
    "selected_units = selected_units.restrict(extended_flashes) \n",
    "\n",
    "# Filter according to criterion 3\n",
    "selected_units = selected_units[(selected_units[\"rate\"]>2.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd12fce1",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "assert len(selected_units) == 92"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb10538",
   "metadata": {},
   "source": [
    "Now, in order to determine the responsiveness of the units, it's helpful to use the {func}`~pynapple.process.perievent.compute_perievent` function: this will align units' spiking timestamps with the onset of the stimulus repetitions and take an average over them.\n",
    "\n",
    "Let's use that function to construct two separate perievent dictionaries, one aligned to the start of the white stimuli, one aligned to the start of the black, and they should run from 250 msec before to 500 msec after the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set window of perievent 500 ms before and after the start of the event\n",
    "window_size = (-.250, .500) \n",
    "\n",
    "# Re-center timestamps for white stimuli\n",
    "# +50 because we subtracted 500 ms at beginning of stimulus presentation\n",
    "peri_white = nap.compute_perievent(timestamps=selected_units,\n",
    "                                   tref=nap.Ts(extended_flashes_white.start +.50), \n",
    "                                   minmax=window_size)\n",
    "\n",
    "# Re-center timestamps for black stimuli\n",
    "# +50 because we subtracted 500 ms at beginning of stimulus presentation\n",
    "peri_black = nap.compute_perievent(timestamps=selected_units,\n",
    "                                   tref=nap.Ts(extended_flashes_black.start +.50), \n",
    "                                   minmax=window_size)\n",
    "\n",
    "# OR\n",
    "\n",
    "peri_white = nap.compute_perievent(timestamps=selected_units,\n",
    "                                   tref=nap.Ts(flashes_white.start),\n",
    "                                   minmax=window_size)\n",
    "peri_black = nap.compute_perievent(timestamps=selected_units,\n",
    "                                   tref=nap.Ts(flashes_black.start),\n",
    "                                   minmax=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(peri_white) == len(selected_units)\n",
    "assert ([p.ref_times for p in peri_white.values()] == flashes_white.start).all()\n",
    "assert len(peri_black) == len(selected_units)\n",
    "assert ([p.ref_times for p in peri_black.values()] == flashes_black.start).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619548ab",
   "metadata": {},
   "source": [
    "Visualizing these perievents can help us determine which units to include. The following helper function should help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb0c0e",
   "metadata": {
    "tags": [
     "render-all",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_raster_psth(peri, units, color_flashes, n_units=9, start_unit=0, bin_size=.005, smoothing=0.015):\n",
    "    \"\"\"\n",
    "    Plot perievent time histograms (PSTHs) and raster plots for multiple units.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    peri : dict\n",
    "        Dictionary mapping unit names to binned spike count peri-stimulus data (e.g., binned time series).\n",
    "    units : dict\n",
    "        Dictionary of neural units, e.g., spike trains or trial-aligned spike events.\n",
    "    color_flashes : str\n",
    "        A label indicating the flash color condition ('black' or 'white'), used for visual styling.\n",
    "    n_units : int\n",
    "        The number of units to visualize.\n",
    "    start_unit : int\n",
    "        The index of the unit to start with.\n",
    "    bin_size : float\n",
    "        Size of the bin used for spike count computation (in seconds).\n",
    "    smoothing : float\n",
    "        Standard deviation for Gaussian smoothing of the PSTH traces.\n",
    "    \"\"\"\n",
    "\n",
    "    # Layout setup: 9 columns (units), 2 rows (split vertically into PSTH and raster plot)\n",
    "    n_cols = n_units\n",
    "    n_rows = 2\n",
    "    fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                             figsize=(n_cols*2, 4))\n",
    "\n",
    "    colors = plt.cm.tab10.colors\n",
    "\n",
    "    # Extract unit names for iteration\n",
    "    units_list = list(units.keys())[start_unit:start_unit+n_units]\n",
    "\n",
    "    for i, unit in enumerate(units_list):\n",
    "        u = peri[unit]\n",
    "        line_color = colors[i % len(colors)]\n",
    "        ax = axes[0, i]\n",
    "\n",
    "        # Plot PSTH (smoothed firing rate)\n",
    "        ax.plot(\n",
    "            (np.mean(u.count(bin_size), 1) / bin_size).smooth(std=smoothing),\n",
    "            linewidth=2,\n",
    "            color=line_color\n",
    "        )\n",
    "        ax.axvline(0.0)  # Stimulus onset line\n",
    "\n",
    "        span_color = \"black\" if color_flashes == \"black\" else \"silver\"\n",
    "        ax.axvspan(0, 0.250, color=span_color, alpha=0.3, ec=\"black\")  # Stim duration\n",
    "        ax.set_xlim(-0.25, 0.50)\n",
    "        ax.set_title(f'{unit}')\n",
    "\n",
    "        # Plot raster\n",
    "        ax = axes[1, i]\n",
    "        ax.plot(u.to_tsd(), \"|\", markersize=1, color=line_color, mew=2)\n",
    "        ax.axvline(0.0)\n",
    "        ax.axvspan(0, 0.250, color=span_color, alpha=0.3, ec=\"black\")\n",
    "        ax.set_ylim(0, 75)\n",
    "        ax.set_xlim(-0.25, 0.50)\n",
    "\n",
    "    # Y-axis and title annotations\n",
    "    axes[0, 0].set_ylabel(\"Rate (Hz)\")\n",
    "    axes[1, 0].set_ylabel(\"Trial\")\n",
    "    if n_rows > 2:\n",
    "        axes[2, 0].set_ylabel(\"Rate (Hz)\")\n",
    "        axes[3, 0].set_ylabel(\"Trial\")\n",
    "    fig.text(0.5, 0.00, 'Time from stim(s)', ha='center')\n",
    "    fig.text(0.5, 1.00, f'PSTH & Spike Raster Plot - {color_flashes} flashes', ha='center')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98b166",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "# called like this, the function will visualize the first 9 units. play with the n_units\n",
    "# and start_unit arguments to see the other units.\n",
    "plot_raster_psth(peri_white, selected_units, \"white\", n_units=9, start_unit=0)\n",
    "plot_raster_psth(peri_black, selected_units, \"black\", n_units=9, start_unit=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721b205",
   "metadata": {},
   "source": [
    "<div class=\"render-all\">\n",
    "\n",
    "You could manually visualize each of our units and select those that appear, from their PSTH to be responsive.\n",
    "\n",
    "However, it would be easier to scale (and more reproducible) if you came up with some measure of responsiveness. So how do we compute something that captures \"this neuron responds to visual stimuli\"?\n",
    "\n",
    "You should be able to do this using a function that iterates over the `peri_white` and `peri_black` dictionaries, returning a single float for each unit.\n",
    "\n",
    "Let's aim to pick around 20 neurons.\n",
    "\n",
    "If you're having trouble coming up with one that seems reasonable, expand the following admonition.\n",
    "\n",
    ":::{admonition} How to compute responsiveness?\n",
    ":class: hint dropdown\n",
    "\n",
    "Try defining responsiveness as the normalized difference in average firing rate between during stimulus presentation and before the stimulus was presented.\n",
    "\n",
    "We can use {func}`~pynapple.TsGroup.restrict` together with `np.mean` to compute the average firing rates above, and then combine them.\n",
    "\n",
    ":::\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee63f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responsiveness(perievents, bin_size=0.005):\n",
    "    \"\"\"Calculate responsiveness for each neuron. This is computed as:\n",
    "\n",
    "    post_presentation_avg  : \n",
    "        Average firing rate during presentation (250 ms) of stimulus across\n",
    "        all instances of stimulus. \n",
    "\n",
    "    pre_presentation_avg :\n",
    "        Average firing rate prior (250 ms) to the presentation of stimulus\n",
    "        across all instances prior of stimulus. \n",
    "\n",
    "    responsiveness : \n",
    "        abs((post_presentation_avg - pre_presentation_avg) / (post_presentation_avg + pre_presentation_avg))\n",
    "\n",
    "    Larger values indicate higher responsiveness to the stimuli.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    perievents : TsGroup\n",
    "        Contains perievent information of a subset of neurons\n",
    "    bin_size : float\n",
    "        Bin size for calculating spike counts\n",
    "\n",
    "    Returns\n",
    "    ----------   \n",
    "    resp_array : np.array\n",
    "        Array of responsiveness information.\n",
    "\n",
    "    \"\"\"\n",
    "    resp_dict = {}\n",
    "    resp_array = np.zeros(len(perievents.keys()), dtype=float)\n",
    "\n",
    "    for index, peri in enumerate(perievents.values()):\n",
    "        # Count the number of timestamps in each bin_size bin.\n",
    "        peri_counts = peri.count(bin_size)\n",
    "\n",
    "        # Compute average spikes for each millisecond in the\n",
    "        # the 250 ms before stimulus presentation\n",
    "        pre_presentation = np.mean(peri_counts, 1).restrict(nap.IntervalSet([-.25,0]))\n",
    "\n",
    "        # Compute average spikes for each millisecond in the\n",
    "        # the 250 ms after stimulus presentation\n",
    "        post_presentation = np.mean(peri_counts, 1).restrict(nap.IntervalSet([0,.25]))\n",
    "\n",
    "        pre_presentation_avg = np.mean(pre_presentation)\n",
    "        post_presentation_avg = np.mean(post_presentation)\n",
    "        responsiveness = abs((post_presentation_avg - pre_presentation_avg) / (post_presentation_avg + pre_presentation_avg))\n",
    "\n",
    "        resp_array[index] = responsiveness\n",
    "\n",
    "    return resp_array\n",
    "\n",
    "\n",
    "responsiveness_white = get_responsiveness(peri_white)\n",
    "responsiveness_black = get_responsiveness(peri_black)\n",
    "\n",
    "# Get threshold for top 15% most responsive\n",
    "thresh_black = np.percentile(responsiveness_black, 85)\n",
    "thresh_white = np.percentile(responsiveness_white, 85)\n",
    "\n",
    "# Only keep units that are within the 15% most responsive for either black or white\n",
    "selected_units = selected_units[(responsiveness_black > thresh_black) | (responsiveness_white > thresh_white)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff57d55",
   "metadata": {},
   "source": [
    "Let's visualize the selected units PSTHs to make sure they all look reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d136e",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"Remaining units: {len(selected_units)}\")\n",
    "peri_white = {k: peri_white[k] for k in selected_units.index}\n",
    "peri_black = {k: peri_black[k] for k in selected_units.index}\n",
    "\n",
    "plot_raster_psth(peri_black, selected_units, \"black\", n_units=len(peri_black))\n",
    "plot_raster_psth(peri_white, selected_units, \"white\", n_units=len(peri_white))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333e75f",
   "metadata": {},
   "source": [
    "## Avoiding overfitting\n",
    "\n",
    "\n",
    "\n",
    "As we've seen throughout this workshop, it is important to avoid overfitting your model. We've covered two strategies for doing so: either separate your dataset into train and test subsets or set up a cross-validation scheme. Pick one of these approaches and use it when fitting your GLM model in the next section.\n",
    "\n",
    "You might find it helpful to refer back to the [](sklearn) notebook and / or to use the following pynapple functions: {func}`~pynapple.IntervalSet.set_diff`, {func}`~pynapple.IntervalSet.union`, {func}`~pynapple.TsGroup.restrict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take one every three flashes (33% of all flashes of test)\n",
    "flashes_test_white = extended_flashes_white[::3]\n",
    "flashes_test_black = extended_flashes_black[::3]\n",
    "# The remaining is separated for training\n",
    "flashes_train_white = extended_flashes_white.set_diff(flashes_test_white)\n",
    "flashes_train_black = extended_flashes_black.set_diff(flashes_test_black)\n",
    "# Merge both stimuli types in a single interval set\n",
    "flashes_test = flashes_test_white.union(flashes_test_black)\n",
    "flashes_train = flashes_train_white.union(flashes_train_black)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062368a",
   "metadata": {},
   "source": [
    "## Fit a GLM\n",
    "\n",
    "\n",
    "\n",
    "In this section, you will use nemos to build a GLM. There are a lot of scientific decisions to be made here, so we suggest starting simple and then adding complexity. Construct a design matrix with a single predictor, using a basis of your choice, then construct, fit, and score your model to a single neuron (remembering to either use your train/test or cross-validation to avoid overfitting). Then add regularization to your GLM. Then return to the beginning and add more predictors. Then fit all the neurons. Then evaluate what basis functions and parameters are best for your predictors. Then use the tricks we covered in [](sklearn) to evaluate whether which predictors are necessary for your model, which are the most important.\n",
    "\n",
    "You don't have to exactly follow those steps, but make sure you can go from beginning to end before getting too complex.\n",
    "\n",
    "Good luck and we look forward to seeing what you come up with!\n",
    "\n",
    "\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "\n",
    "\n",
    "- Create spike count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General spike counts\n",
    "bin_size = .005\n",
    "units_counts = selected_units.count(bin_size, ep=extended_flashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60930b",
   "metadata": {},
   "source": [
    "### Construct design matrix\n",
    "\n",
    "<div class=\"render-all\">\n",
    "\n",
    "- Decide on feature(s)\n",
    "- Decide on basis\n",
    "- Construct design matrix\n",
    "\n",
    ":::{admonition} What features should I include?\n",
    ":class: hint dropdown\n",
    "\n",
    "If you're having trouble coming up with features to include, here are some possibilities:\n",
    "- Stimulus.\n",
    "- Stimulus onset.\n",
    "- Stimulus offset.\n",
    "- For multiple neurons: neuron-to-neuron coupling.\n",
    "\n",
    "For the stimuli predictors, you probably want to model white and black separately.\n",
    "\n",
    ":::\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TsdFrame filled by zeros, for the size of units_counts\n",
    "stim = nap.TsdFrame(\n",
    "    t=units_counts.t,\n",
    "    d=np.zeros((len(units_counts), 2)), \n",
    "    columns = ['white', 'black']\n",
    ")\n",
    "# Check whether there is a flash within a given bin of spikes\n",
    "# If there is not, put a nan in that index\n",
    "idx_white = flashes_white.in_interval(units_counts)\n",
    "idx_black = flashes_black.in_interval(units_counts)\n",
    "\n",
    "# Replace everything that is not nan with 1 in the corresponding column\n",
    "stim.d[~np.isnan(idx_white), 0] = 1\n",
    "stim.d[~np.isnan(idx_black), 1] = 1\n",
    "\n",
    "white_onset = nap.Tsd(\n",
    "    t=stim.t, \n",
    "    d=np.hstack((0,np.diff(stim[\"white\"])==1)),\n",
    "    time_support = units_counts.time_support\n",
    ")\n",
    "\n",
    "white_offset = nap.Tsd(\n",
    "    t=stim.t, \n",
    "    d=np.hstack((0,np.diff(stim[\"white\"])==-1)),\n",
    "    time_support = units_counts.time_support\n",
    ")\n",
    "\n",
    "black_onset = nap.Tsd(\n",
    "    t=stim.t, \n",
    "    d=np.hstack((0,np.diff(stim[\"black\"])==1)),\n",
    "    time_support = units_counts.time_support\n",
    ")\n",
    "black_offset = nap.Tsd(\n",
    "    t=stim.t, \n",
    "    d=np.hstack((0,np.diff(stim[\"black\"])==-1)),\n",
    "    time_support = units_counts.time_support\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdc7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration of stimuli\n",
    "stimulus_history_duration = 0.250\n",
    "\n",
    "# Window length in bin size units\n",
    "window_len = int(stimulus_history_duration / bin_size)\n",
    "\n",
    "# Initialize basis objects\n",
    "basis_white_on = nmo.basis.RaisedCosineLogConv(\n",
    "    n_basis_funcs=5, \n",
    "    window_size=window_len, \n",
    "    label=\"white_on\"\n",
    ")\n",
    "basis_white_off = nmo.basis.RaisedCosineLinearConv(\n",
    "    n_basis_funcs=5, \n",
    "    window_size=window_len, \n",
    "    label=\"white_off\",\n",
    "    conv_kwargs={\"predictor_causality\":\"acausal\"}\n",
    ")\n",
    "basis_white_stim= nmo.basis.RaisedCosineLogConv(\n",
    "    n_basis_funcs=5, \n",
    "    window_size=window_len, \n",
    "    label=\"white_stim\"\n",
    ")\n",
    "basis_black_on = nmo.basis.RaisedCosineLogConv(\n",
    "    n_basis_funcs=5, \n",
    "    window_size=window_len, \n",
    "    label=\"black_on\"\n",
    ")\n",
    "basis_black_off = nmo.basis.RaisedCosineLinearConv(\n",
    "    n_basis_funcs=5, \n",
    "    window_size=window_len, \n",
    "    label=\"black_off\",\n",
    "    conv_kwargs={\"predictor_causality\":\"acausal\"}\n",
    ")\n",
    "basis_black_stim = nmo.basis.RaisedCosineLogConv(\n",
    "    n_basis_funcs=5, \n",
    "    window_size=window_len, \n",
    "    label=\"black_stim\"\n",
    ")\n",
    "\n",
    "# Define additive basis object to construct design matrix\n",
    "additive_basis = (\n",
    "    basis_white_on + \n",
    "    basis_white_off + \n",
    "    basis_white_stim + \n",
    "    basis_black_on + \n",
    "    basis_black_off + \n",
    "    basis_black_stim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve basis with inputs - test set\n",
    "X_test = additive_basis.compute_features(\n",
    "    white_onset.restrict(flashes_test),\n",
    "    white_offset.restrict(flashes_test),\n",
    "    stim[\"white\"].restrict(flashes_test),\n",
    "    black_onset.restrict(flashes_test),\n",
    "    black_offset.restrict(flashes_test),\n",
    "    stim[\"black\"].restrict(flashes_test)\n",
    ")\n",
    "\n",
    "# Convolve basis with inputs - train set\n",
    "X_train = additive_basis.compute_features(\n",
    "    white_onset.restrict(flashes_train),\n",
    "    white_offset.restrict(flashes_train),\n",
    "    stim[\"white\"].restrict(flashes_train),\n",
    "    black_onset.restrict(flashes_train),\n",
    "    black_offset.restrict(flashes_train),\n",
    "    stim[\"black\"].restrict(flashes_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23690e",
   "metadata": {},
   "source": [
    "### Construct and fit your model\n",
    "\n",
    "\n",
    "\n",
    "- Decide on regularization\n",
    "- Initialize GLM\n",
    "- Call fit\n",
    "- Visualize result on PSTHs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc468530",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer_strength = 7.745e-06\n",
    "# Initialize model object of a single unit\n",
    "model = nmo.glm.GLM(\n",
    "    regularizer=\"Ridge\",\n",
    "    regularizer_strength=regularizer_strength,\n",
    "    solver_name=\"LBFGS\", \n",
    ")\n",
    "# Choose an example unit\n",
    "unit_id = 951768318\n",
    "\n",
    "# Get counts for train and test for said unit\n",
    "u_counts = units_counts.loc[unit_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe5491",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, u_counts.restrict(flashes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use predict to obtain the firing rates\n",
    "pred_unit = model.predict(X_test)\n",
    "\n",
    "# Convert units from spikes/bin to spikes/sec\n",
    "pred_unit = pred_unit/ bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba48269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-center timestamps around white stimuli\n",
    "# +50 because we subtracted .50 at beginning of stimulus presentation\n",
    "peri_white_pred_unit = nap.compute_perievent_continuous(\n",
    "    timeseries=pred_unit, \n",
    "    tref=nap.Ts(flashes_test_white.start+.50),\n",
    "    minmax=window_size\n",
    ")  \n",
    "# Re-center timestamps for black stimuli\n",
    "# +50 because we subtracted .50 at beginning of stimulus presentation\n",
    "peri_black_pred_unit = nap.compute_perievent_continuous(\n",
    "    timeseries=pred_unit, \n",
    "    tref=nap.Ts(flashes_test_black.start+.50), \n",
    "    minmax=window_size\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edf81f",
   "metadata": {},
   "source": [
    "Here's a helper function for plotting the PSTH of the data and predictions (for one or multiple neurons), which you may find helpful for visualizing your model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017722a",
   "metadata": {
    "tags": [
     "render-all",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_pop_psth(\n",
    "        peri,\n",
    "        color_flashes,\n",
    "        unit_id=None,\n",
    "        bin_size=0.005,\n",
    "        smoothing=0.015,\n",
    "        **peri_others\n",
    "        ):\n",
    "    \"\"\"Plot perievent time histograms (PSTHs) and raster plots for multiple units.\n",
    "\n",
    "    Model predictions should be passed as additional keyword arguments. The key will be\n",
    "    used as the label, and the value should be a 2-tuple of `(style, peri)`, where\n",
    "    `style` is a matplotlib style (e.g., \"blue\" or \"--\") and `peri` is a PSTH\n",
    "    dictionary, as returned by `compute_perievent_continuous`.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    peri : dict or TsGroup\n",
    "        Dictionary mapping unit names to binned spike count peri-stimulus data (e.g., binned time series).\n",
    "    color_flashes : str\n",
    "        A label indicating the flash color condition ('black' or other), used for visual styling.\n",
    "    bin_size : float\n",
    "        Size of the bin used for spike count computation (in seconds).\n",
    "    smoothing : float\n",
    "        Standard deviation for Gaussian smoothing of the PSTH traces.\n",
    "    peri_others : tuple\n",
    "        Model PSTHs to plot. See above for description\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(peri, dict):\n",
    "        peri = {0: peri}\n",
    "        \n",
    "    n_cols = len(peri)\n",
    "    fig, axes = plt.subplots(1, n_cols,\n",
    "                             figsize=(2.5 * n_cols, 2.5))\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, (unit, u) in enumerate(peri.items()):\n",
    "        try:\n",
    "            ax = axes[i]\n",
    "        except TypeError:\n",
    "            # then there's only set of axes\n",
    "            ax = axes\n",
    "        # Plot PSTH (smoothed firing rate)\n",
    "        ax.plot(\n",
    "            (np.mean(u.count(bin_size), 1) / bin_size).smooth(std=smoothing),\n",
    "            linewidth=2,\n",
    "            color=\"black\",\n",
    "            label=\"Observed\"\n",
    "        )\n",
    "        ax.axvline(0.0)  # Stimulus onset line\n",
    "        span_color = \"black\" if color_flashes == \"black\" else \"silver\"\n",
    "        ax.axvspan(0, 0.250, color=span_color, alpha=0.3, ec=\"black\")  # Stim duration\n",
    "        ax.set_xlim(-0.25, 0.50)\n",
    "        ax.set_title(f'{unit}')\n",
    "        for (key, (color, peri_pred)) in peri_others.items():\n",
    "            try:\n",
    "                p = peri_pred[:, :, i]\n",
    "            except IndexError:\n",
    "                p = peri_pred\n",
    "            ax.plot(\n",
    "            (np.mean(p, axis=1)),\n",
    "            linewidth=1.5,\n",
    "            color=color,\n",
    "            label=key.capitalize()\n",
    "            )\n",
    "\n",
    "    # Y-axis and title annotations\n",
    "    axes[0].set_ylabel(\"Rate (Hz)\")\n",
    "    fig.legend(*ax.get_legend_handles_labels())\n",
    "    fig.text(0.5, 0.00, 'Time from stim(s)', ha='center')\n",
    "    fig.text(0.5, 1.00, f'PSTH - {color_flashes} flashes', ha='center')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9600dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pop_psth(peri_white[unit_id], \"white\", predictions=(\"red\", peri_white_pred_unit))\n",
    "plot_pop_psth(peri_black[unit_id], \"black\", predictions=(\"red\", peri_black_pred_unit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee7eb1",
   "metadata": {},
   "source": [
    "### Score your model\n",
    "\n",
    "\n",
    "\n",
    "- We trained on the train set, so now we score on the test set. (Or use cross-validation.)\n",
    "- Get a score for your model that you can use to compare across the modeling choices outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean score for the Stimuli + Coupling model\n",
    "# using pseudo-r2-McFadden\n",
    "score = model.score(\n",
    "    X_test,\n",
    "    u_counts.restrict(flashes_test),\n",
    "    score_type=\"pseudo-r2-McFadden\"\n",
    ")\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f9011",
   "metadata": {},
   "source": [
    "### Try to improve your model?\n",
    "\n",
    "\n",
    "\n",
    "- Go back to the beginning of [this section](visual-glm) and try to improve your model's performance (as reflected by increased score).\n",
    "- Keep track of what you've tried and their respective scores. \n",
    "    - You can do this by hand, but constructing a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), as we've seen in [](sklearn), is useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99415a4b",
   "metadata": {
    "tags": [
     "render-all"
    ]
   },
   "outputs": [],
   "source": [
    "# Example construction of dataframe.\n",
    "# In this:\n",
    "# - additive_basis is the single AdditiveBasis object we used to construct the entire design matrix\n",
    "# - model is the GLM we fit to a single neuron\n",
    "# - unit_id is the int identifying the neuron we're fitting\n",
    "# - score is the float giving the model score, summarizing model performance (on the test set)\n",
    "import pandas as pd\n",
    "data = [\n",
    "    {\n",
    "        \"model_id\": 0,\n",
    "        \"regularizer\": model.regularizer.__class__.__name__,\n",
    "        \"regularizer_strength\": model.regularizer_strength,\n",
    "        \"solver\": model.solver_name,\n",
    "        \"score\": score,\n",
    "        \"n_predictors\": len(additive_basis),\n",
    "        \"unit\": unit_id,\n",
    "        \"predictor_i\": i,\n",
    "        \"predictor\": basis.label.strip(),\n",
    "        \"basis\": basis.__class__.__name__,\n",
    "        # any other info you think is important ...\n",
    "    }\n",
    "    for i, basis in enumerate(additive_basis)\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.18.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   27,
   63,
   85,
   95,
   123,
   135,
   144,
   159,
   181,
   195,
   201,
   209,
   216,
   224,
   240,
   252,
   274,
   285,
   289,
   299,
   325,
   330,
   338,
   411,
   418,
   443,
   506,
   514,
   523,
   535,
   545,
   567,
   571,
   596,
   636,
   688,
   708,
   721,
   736,
   740,
   748,
   763,
   771,
   850,
   853,
   864,
   874,
   886
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}